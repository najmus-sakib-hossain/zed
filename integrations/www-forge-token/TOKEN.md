Let me research every major claim in your token-saving plan against real-world evidence.Now I have comprehensive, real-world data on every major technique in your plan. Here's the brutal truth on each one.

---

# üî¨ THE BRUTAL TRUTH: Your 25 Token-Saving Crates ‚Äî What's Real, What's Hype, What's Hallucination

---

## The Verdict Scale

- ‚úÖ **REAL** ‚Äî Backed by research, documentation, and production evidence
- ‚ö†Ô∏è **PARTLY REAL** ‚Äî The technique works, but your claimed savings % is inflated
- üö® **HALLUCINATION** ‚Äî The concept is flawed, the savings are fabricated, or it'll backfire
- ü§∑ **DEPENDS** ‚Äî Highly context-dependent; could be 0% or could be huge

---

## Crate-by-Crate Verdict

### ‚úÖ Crate 1: `prefix-cache` ‚Äî Claim: "50% on cached input tokens"

**VERDICT: REAL.** This is one of your strongest crates.

By reusing recently seen input tokens, developers can get a 50% discount and faster prompt processing times. Prompt Caching can reduce latency by up to 80% and input token costs by up to 90%. Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees.

However, there are critical constraints your crate MUST handle:

Caching is enabled automatically for prompts that are 1024 tokens or longer. Cache hits require an exact, repeated prefix match. Caches generally last 5-10 minutes of inactivity up to one hour.

And the savings now vary by model: Discount varies by model: GPT-5 family (90% off), GPT-4.1 family (75% off), GPT-4o/O-series (50% off).

**Brutal correction:** Your claim of "50%" is actually **underselling** it for newer models. GPT-5 gives **90% off** cached tokens. But the 1024-token minimum and 5-10 minute expiry are real constraints. Your crate's value is in **guaranteeing byte-for-byte prefix stability** ‚Äî that's genuinely useful. ‚úÖ

---

### ‚ö†Ô∏è Crate 2: `compaction` ‚Äî Claim: "50-84% history"

**VERDICT: PARTLY REAL.** Compaction works, but the quality trade-off is real.

Practical rule: Leverage your evals to choose the compaction method and frequency that balances cost (both from reducing total input tokens via truncation/summarization as well as caching) and intelligence gained from careful context engineering.

Aggressive compaction breaks the model's ability to recall prior decisions. The 84% claim assumes you're throwing away 84% of conversation history ‚Äî which destroys context in complex agent tasks. Realistic useful compaction without quality loss: **30-50%**, not 84%.

---

### ‚úÖ Crate 3: `governor` ‚Äî Claim: "prevents waste"

**VERDICT: REAL.** Circuit breakers for tool calls are pure engineering ‚Äî no hype involved. If a tool loops 5 times reading the same file, stopping it saves real tokens. The only risk is being too aggressive and blocking legitimate retries.

---

### ‚úÖ Crate 4: `reasoning-router` ‚Äî Claim: "30-80% reasoning"

**VERDICT: REAL.** This is one of the most impactful techniques in 2025-2026.

O-series models use 'reasoning tokens' for internal thinking that are billed as output tokens but not returned in the response. This means actual costs can be significantly higher than estimated based on visible output. A response showing 500 output tokens may actually consume 2000+ tokens. Monitor O-series usage carefully and avoid using for simple tasks where GPT-5/GPT-4.1 models would suffice.

Routing simple tasks to non-reasoning models saves massive amounts. Using `reasoning_effort: "low"` vs `"high"` on o-series models can genuinely save 30-80% on reasoning tokens. ‚úÖ

---

### ‚úÖ Crate 5: `vision-compress` ‚Äî Claim: "70-96% images"

**VERDICT: REAL.** This is backed directly by OpenAI's own documentation.

You can save tokens and speed up responses by using "detail": "low". This lets the model process the image with a budget of 85 tokens.

Compare: 4 512px square tiles are needed to represent the image, so the final token cost is 170 * 4 + 85 = 765.

So a 1024√ó1024 image at high detail = **765 tokens**. Same image at low detail = **85 tokens**. That's an **89% reduction**. For a 2048√ó4096 image: the final token cost is 170 * 6 + 85 = 1105. Down to 85 at low detail = **92% reduction**. Your 70-96% claim is **actually correct**. ‚úÖ

---

### ‚ö†Ô∏è Crate 6: `ocr-extract` ‚Äî Claim: "100% images‚Üítext"

**VERDICT: PARTLY REAL.** The concept is sound ‚Äî OCR a text-heavy screenshot instead of sending it as an image. But "100%" is misleading. You save 100% of *image* tokens but you ADD text tokens for the OCR output. Net savings depend on image size vs text density. For a screenshot of code: high-detail image might be ~1000 tokens, OCR text might be ~200 tokens = **80% savings**. For a photo with little text: OCR is useless and you've wasted compute. The 100% claim should be **60-90% on text-heavy images, 0% on photos**.

---

### ‚ö†Ô∏è Crate 7: `semantic-cache` ‚Äî Claim: "100% per hit"

**VERDICT: REAL PER HIT, but hit rates are low for agents.**

The technique is proven: Teams using semantic caching typically cut their LLM costs by 50% or more, depending on how repetitive their query patterns are. The more similar questions your users ask, the bigger the savings.

But real-world hit rates vary dramatically: Early tests for Portkey's semantic cache reveal a promising ~20% cache hit rate at 99% accuracy for Q&A (or RAG) use cases.

For a customer support chatbot: hit rates of 20-69% are documented. cache hit rates ranging from 61.6% to 68.8% ‚Äî but that's for repetitive support queries.

For a **coding agent** (your DX use case), hit rates will be **5-15%** at best because every task is different. And there are real risks: Unlike traditional caching, semantic caches introduce new risks: Sudden model updates change embeddings and break matches. Vector drift causes cache misses even for similar queries.

**Brutal correction:** "100% per hit" is technically true but misleading. For agents, expect **5-15% hit rate** = **5-15% overall savings**, not 100%.

---

### ‚ö†Ô∏è Crate 8: `schema-minifier` ‚Äî Claim: "40-70% schemas"

**VERDICT: PARTLY REAL, but risky.** Stripping descriptions from tool schemas saves tokens, but models use those descriptions to decide *when* to call tools. Removing them can cause the model to misuse tools or fail to call them. Safe minification (removing defaults, extra whitespace, examples) gets you **20-35%** savings. Stripping descriptions gets you 40-70% but may degrade tool selection quality. Be conservative.

---

### ‚ö†Ô∏è Crate 9: `output-truncator` ‚Äî Claim: "50-95% outputs"

**VERDICT: PARTLY REAL.** Head+tail truncation on long tool outputs (e.g., a 5000-line `ls -la` output) is absolutely valid and saves huge amounts. But 95% truncation is extreme ‚Äî you're showing the model 5% of the data. Works for obvious cases (huge file listings, error logs). Dangerous for code files where the middle matters. Realistic safe savings: **30-60%** on genuinely long outputs.

---

### ‚úÖ Crate 10: `dedup` ‚Äî Claim: "20-50% dupes"

**VERDICT: REAL.** Agents frequently re-read the same file or re-run the same command. Deduplicating identical tool outputs is pure win with zero quality loss. The 20-50% range is honest for agentic workflows. ‚úÖ

---

### ‚úÖ Crate 11: `retrieval-budget` ‚Äî Claim: "60-90% retrieval"

**VERDICT: REAL.** Capping the number of retrieved chunks before stuffing them into context is a well-established RAG optimization. LongLLMLingua reduces costs and boosts efficiency with prompt compression, improving RAG performance by up to 21.4% using only 1/4 of the tokens. This technique actually **improves** quality by reducing noise. ‚úÖ

---

### ‚úÖ Crate 12: `patch-prefer` ‚Äî Claim: "90-98% edits"

**VERDICT: REAL.** Instructing the model to output diffs instead of full files is the single highest-impact token saver for coding agents. A 500-line file with a 3-line change: full file = ~2000 tokens, diff = ~40 tokens = **98% savings**. This is not hype ‚Äî it's math. ‚úÖ

---

### ‚úÖ Crate 13: `context-pruner` ‚Äî Claim: "20-40% history"

**VERDICT: REAL.** Removing stale tool outputs (file reads from 10 turns ago that are no longer relevant) is safe and effective. 20-40% is an honest range. ‚úÖ

---

### ‚ö†Ô∏è Crate 14: `rlm` ‚Äî Claim: "up to 90% big files"

**VERDICT: PARTLY REAL, but the 90% is misleading.** The Recursive Language Model approach (send an index instead of the full file) works ‚Äî but the model often needs to then request the actual content chunks, which costs *additional* API calls and tokens. You save 90% on the *initial* send but may pay it back in follow-up reads. Net savings: **40-70%** for genuinely huge files, **negative** for small files where the index adds overhead. The threshold_tokens config is critical.

---

### ‚úÖ Crate 15: `batch-router` ‚Äî Claim: "50% cost"

**VERDICT: REAL.** This is directly from OpenAI's pricing:

Better cost efficiency: 50% cost discount compared to synchronous APIs. Higher rate limits. Fast completion times: Each batch completes within 24 hours (and often more quickly).

Batch API: Save 50% on inputs and outputs with the Batch API and run tasks asynchronously over 24 hours.

The 50% is a **hard fact from OpenAI's pricing page**. ‚úÖ But your keyword-matching approach for detecting batch-eligible tasks is crude ‚Äî it'll miss some and misclassify others. Still, the underlying savings are real.

---

### ‚úÖ Crate 16: `tool-router` ‚Äî Claim: "50-90% schemas"

**VERDICT: REAL.** If you have 50 tools and only send 5 relevant ones, you literally save 90% of tool schema tokens. The entire request prefix is cacheable: messages, images, audio, tool definitions, and structured output schemas. Fewer tools = smaller prefix = better cache hits too. However, your keyword-based routing is fragile. A smarter approach would use the model's own context or a lightweight classifier. ‚úÖ on concept, ‚ö†Ô∏è on implementation.

---

### ‚ö†Ô∏è Crate 17: `prompt-compress` ‚Äî Claim: "15-40% on verbose prompts"

**VERDICT: PARTLY REAL, but your implementation is too naive.**

Microsoft's LLMLingua achieves real compression: LLMLingua was able to retain the reasoning capabilities of LLMs at a 20x compression ratio, with only a 1.5% loss in performance. Our method achieves state-of-the-art performance across all datasets, with up to 20x compression with only a 1.5 point performance drop.

But LLMLingua uses a **neural model** (GPT-2/LLaMA-7B) to identify which tokens to remove. Your implementation just removes filler phrases and collapses whitespace ‚Äî that's **whitespace normalization, not prompt compression**. Rule-based filler removal gets you **5-15%**, not 15-40%. For 15-40% you'd need actual perplexity-based token selection, which requires running a small model ‚Äî adding latency and complexity.

**Brutal correction:** Rename this or be honest: your rule-based approach saves **5-15%**, not 15-40%. Real LLMLingua-class compression would save 15-40% but requires a neural model you're not using.

---

### ‚ö†Ô∏è Crate 18: `cot-compress` ‚Äî Claim: "30-60% reasoning"

**VERDICT: PARTLY REAL, but dangerous.** Removing "Let me think..." lines from assistant messages before they enter history works. But your prefix-matching approach is fragile ‚Äî it'll sometimes remove actual conclusions that happen to start with "Looking at" or "Considering". And for reasoning models (o-series), the thinking tokens are already hidden. Net safe savings: **15-30%** with careful implementation, not 30-60%.

---

### ‚ö†Ô∏è Crate 19: `vision-select` ‚Äî Claim: "60-80% images"

**VERDICT: CONCEPT IS REAL, implementation is naive.** The two-pass approach (low-detail overview ‚Üí high-detail crops of ROIs) is theoretically sound. But your grid-based edge density detector is extremely crude compared to what's actually needed. Many important image regions (text, UI elements) have low edge density. And you're adding *multiple* images (overview + crops) which has its own overhead. This needs serious testing before claiming 60-80%.

---

### ‚úÖ Crate 20: `response-cache` ‚Äî Claim: "100% per hit"

**VERDICT: REAL for exact/near-exact matches.** Persistent disk caching with blake3 hashing is solid engineering. Unlike semantic cache, this uses deterministic hashing ‚Äî zero false positive risk. The savings per hit are genuinely 100% (skipped API call). Hit rates for an agent will be low (5-10%) but the implementation cost is also low. ‚úÖ

---

### ‚úÖ Crate 21: `token-budget` ‚Äî Claim: "prevents overflow"

**VERDICT: REAL.** This is defensive engineering, not a savings technique. It prevents the catastrophic case of exceeding context windows (which causes retries or errors). Using tiktoken for accurate counting is correct. ‚úÖ

---

### ‚ö†Ô∏è Crate 22: `history-summarizer` ‚Äî Claim: "60-90% old history"

**VERDICT: PARTLY REAL, but costs tokens to save tokens.** The summarization call itself costs tokens. For a 10,000-token history summarized to 500 tokens: you save 9,500 tokens on subsequent turns but spent ~11,000 tokens (history + prompt to summarizer + summary output) to generate it. You break even after ~2 subsequent turns. Over a 20-turn conversation: genuinely saves a lot. Over a 3-turn conversation: net negative. The 60-90% is the compression ratio, not the net savings.

---

### ‚ö†Ô∏è Crate 23: `embedding-compress` ‚Äî Claim: "30-50% retrieval"

**VERDICT: CONCEPT REAL, implementation too crude.** Your Jaccard word-set similarity is a very rough approximation. Two retrieval chunks about the same topic but with different details would score high similarity and get deduplicated ‚Äî losing important information. Real embedding-based dedup requires actual embeddings (you reference fastembed in the dep graph but don't use it in the code). The savings claim of 30-50% is plausible with proper embeddings, not with Jaccard.

---

### ‚ö†Ô∏è Crate 24: `parallel-tool-merge` ‚Äî Claim: "10-30% tool results"

**VERDICT: MARGINAL.** Message overhead per tool result is ~15-20 tokens (role, tool_call_id, etc.). Merging 5 results saves ~60-80 tokens. That's real but tiny compared to the actual content. The 10-30% claim would only hold if tool outputs are very short. For typical file reads (hundreds of tokens each), you save maybe **3-5%**. Not worth the complexity of breaking tool_call_id associations, which some providers need for proper function-calling behavior.

---

### ‚úÖ Crate 25: `whitespace-normalize` ‚Äî Claim: "5-15% formatting"

**VERDICT: REAL and honest.** BOM removal, CRLF‚ÜíLF, tab‚Üíspaces, trailing whitespace ‚Äî these are measurable, lossless savings. The 5-15% range is honest (heavily formatted code/logs save more; clean text saves less). Zero quality risk. ‚úÖ

---

## üìä THE CORRECTED SAVINGS TABLE

| # | Crate | Your Claim | Honest Reality | Verdict |
|---|-------|-----------|---------------|---------|
| 1 | `prefix-cache` | 50% cached | **50-90%** (varies by model) | ‚úÖ Undersold |
| 2 | `compaction` | 50-84% | **30-50%** safe range | ‚ö†Ô∏è Overclaimed |
| 3 | `governor` | prevents waste | **Genuine** | ‚úÖ Real |
| 4 | `reasoning-router` | 30-80% | **30-80%** | ‚úÖ Real |
| 5 | `vision-compress` | 70-96% | **70-92%** | ‚úÖ Real |
| 6 | `ocr-extract` | 100% | **60-90%** on text images, 0% on photos | ‚ö†Ô∏è Conditional |
| 7 | `semantic-cache` | 100% per hit | **100% per hit**, but **5-15% hit rate** for agents | ‚ö†Ô∏è Misleading |
| 8 | `schema-minifier` | 40-70% | **20-35%** safely | ‚ö†Ô∏è Overclaimed |
| 9 | `output-truncator` | 50-95% | **30-60%** safely | ‚ö†Ô∏è Overclaimed |
| 10 | `dedup` | 20-50% | **20-50%** | ‚úÖ Honest |
| 11 | `retrieval-budget` | 60-90% | **60-80%** | ‚úÖ Real |
| 12 | `patch-prefer` | 90-98% | **90-98%** | ‚úÖ Real |
| 13 | `context-pruner` | 20-40% | **20-40%** | ‚úÖ Honest |
| 14 | `rlm` | up to 90% | **40-70%** net (after follow-up reads) | ‚ö†Ô∏è Overclaimed |
| 15 | `batch-router` | 50% cost | **50% cost** (OpenAI confirmed) | ‚úÖ Hard fact |
| 16 | `tool-router` | 50-90% | **50-90%** schemas | ‚úÖ Real |
| 17 | `prompt-compress` | 15-40% | **5-15%** (rule-based) | ‚ö†Ô∏è Overclaimed 3x |
| 18 | `cot-compress` | 30-60% | **15-30%** safely | ‚ö†Ô∏è Overclaimed |
| 19 | `vision-select` | 60-80% | **Unproven** ‚Äî needs real testing | ü§∑ Unverified |
| 20 | `response-cache` | 100% per hit | **100% per hit**, low hit rate | ‚úÖ Real |
| 21 | `token-budget` | prevents overflow | **Genuine** | ‚úÖ Real |
| 22 | `history-summarizer` | 60-90% | **Net 40-70%** after summarization cost | ‚ö†Ô∏è Ignores cost |
| 23 | `embedding-compress` | 30-50% | **10-30%** with Jaccard | ‚ö†Ô∏è Implementation weak |
| 24 | `parallel-tool-merge` | 10-30% | **3-5%** realistically | üö® Not worth it |
| 25 | `whitespace-normalize` | 5-15% | **5-15%** | ‚úÖ Honest |
| ‚Äî | `serializer` (yours) | 44%+ | **Unverifiable** without benchmarks | ü§∑ Show proof |

---

## üö® THE THREE BIGGEST PROBLEMS WITH THE OVERALL PLAN

### PROBLEM 1: Savings Don't Multiply ‚Äî They Compete

Your pipeline applies 25 savers in sequence. But if whitespace-normalize removes 10% of tokens, then prompt-compress has 10% fewer tokens to work with. If vision-compress reduces an image to 85 tokens, then vision-select has nothing left to optimize. **The total savings are NOT the sum of individual savings.** They're much lower because each saver operates on what's left after the previous one.

Realistic total pipeline savings for a coding agent: **40-60% per session**, not the implied 90%+ from stacking all percentages.

### PROBLEM 2: 25 Crates is Way Too Many for Launch

You said "ship fast." 25 crates is not fast. **7-8 of these crates deliver 90% of the total value.** The rest are diminishing returns that add maintenance burden, test surface area, and pipeline latency.

**The 8 crates that matter most (in priority order):**
1. `prefix-cache` ‚Äî biggest single savings, zero quality risk
2. `patch-prefer` ‚Äî 90-98% on edits, the most common agent action
3. `reasoning-router` ‚Äî massive savings on reasoning tokens
4. `tool-router` ‚Äî 50-90% on schema tokens, improves cache hits
5. `vision-compress` ‚Äî huge if your agent handles images
6. `dedup` ‚Äî 20-50%, zero quality risk
7. `token-budget` ‚Äî prevents catastrophic overflow
8. `whitespace-normalize` ‚Äî 5-15%, zero risk, trivial to implement

Ship those 8. Add the rest post-launch based on real user data showing which conversations actually need compaction, history summarization, etc.

### PROBLEM 3: The `serializer` "14% Better Than TOON" Claim

You claim your serializer is 14% more token-efficient than TOON. This is the foundation of your PostResponse stage. But you haven't shown benchmarks, methodology, or what "14%" means (14% fewer characters? 14% fewer tokens? On what tokenizer? On what data?). Token efficiency varies dramatically by tokenizer ‚Äî a format that's 14% smaller in characters might be 5% or 25% different in actual tokens depending on the model. **Publish your benchmarks or this is marketing, not engineering.**

---

## üéØ THE HONEST SHIP-FIRST PLAN

```
Week 1:   prefix-cache + whitespace-normalize + token-budget
Week 2:   patch-prefer + dedup + tool-router
Week 3:   reasoning-router + vision-compress
Week 4:   Tests, benchmarks, honest README with REAL measured savings

Post-launch based on user data:
          compaction, context-pruner, output-truncator,
          semantic-cache, response-cache, batch-router,
          ocr-extract, schema-minifier
          
Defer or cut:
          parallel-tool-merge (3-5% not worth it)
          vision-select (unproven, needs real research)
          prompt-compress (rename to what it actually is)
          rlm (complex, net savings unclear)
          embedding-compress (Jaccard is too crude)
```

---

## üèÅ BOTTOM LINE

**12 of your 25 crates have claims backed by real evidence.** 10 are overclaimed but have real underlying value at lower percentages. 2-3 are not worth shipping. The pipeline architecture is sound but the savings don't stack the way your table implies.

**Build 8 crates. Ship honest benchmarks. Publish real measured savings from real agent sessions.** That's how you build trust and adoption. ü¶Ä

**Brutal truth (based on real 2026 data as of late February):**

Your overall architecture and many individual techniques **are grounded in proven, production-used methods** and can deliver meaningful token/cost savings (realistically 40-70% in well-tuned agent workflows with repetitive or structured elements, sometimes higher on specific components like caching or vision). The pipeline design (early elimination + staged compression) is smart and aligns with current best practices. However, **not all crates will achieve the advertised savings percentages consistently**, some have notable quality or implementation risks, and the grand total "90%+" compounding effect is optimistic hype for most real-world agent use cases. Aggressive application can hurt agent performance (accuracy, reliability, tool success rate).

Here's the evidence-based breakdown from current sources (provider docs, benchmarks, practitioner reports, and papers from late 2025‚Äìearly 2026):

### Strong / High-Confidence Wins (These Really Work)
- **Prompt caching (prefix-cache)**: Excellent. Anthropic and OpenAI prompt caching deliver 41-90% cost reduction on cached portions (often 70-80% in repetitive agent workflows with stable system prompts or documents). Latency also drops 13-31%. Requires careful prefix stability ‚Äî your crate's focus on byte-for-byte identical prefixes is correct. Real-world agent papers confirm big wins when dynamic content (tool results) is placed after cached blocks.

- **Semantic + persistent response caching (semantic-cache, response-cache)**: Very effective for repetitive queries. Redis-style semantic caching achieves ~73% cost reduction in high-repetition workloads. Persistent disk caches (your redb + zstd approach) complement in-memory ones for cross-session savings. Hit rates vary heavily by domain ‚Äî great for common tasks, lower for unique agent trajectories.

- **Tool routing / schema minification (tool-router, schema-minifier)**: One of the strongest areas. With 50+ tools, full schemas waste thousands of tokens per call. Dynamic selection or minification routinely saves 50-91%+ on tool tokens in practice. Semantic tool selection is a proven pattern in 2026 agent frameworks. Your keyword + core-tools approach is simple and effective.

- **Prompt compression (prompt-compress + your serializer)**: Proven via LLMLingua-style methods. Rule-based (fillers, whitespace, regex) gives reliable 15-40% savings with low risk. Your serializer being 14% better than TOON is plausible for structured data. More advanced ML compression reaches 4-20x in RAG/long-context with minimal quality loss in many cases.

- **Vision optimizations (vision-select, vision-compress, ocr-extract)**: Solid. OCR-first for text-heavy images is standard advice. ROI cropping + downscaling + low-detail overview can reduce vision tokens dramatically (e.g., full high-detail 1920x1080 ~2000 tokens ‚Üí overview + targeted crops ~500-600). DeepSeek-OCR and similar systems show real gains in token efficiency for documents.

- **History summarization + compaction (history-summarizer, compaction, context-pruner)**: Common and effective for long-running agents. Summarization preserves key decisions while cutting old history 60-90%. Rule-based + occasional LLM summarization is a standard pattern.

- **Token budget / pruner / dedup / whitespace-normalize**: Necessary hygiene. Prevent overflows and clean small wastes (5-15% from formatting). Output truncation and parallel merge help post-response bloat.

- **Batch routing**: Real 50% cost discount via OpenAI Batch (or equivalents) for non-urgent background tasks.

### Mixed / Moderate Wins (Work but with Caveats)
- **CoT compression (cot-compress)**: Rule-based prefix detection can save 30-60% on verbose reasoning in history, but risks losing important intermediate steps. Better as a lightweight pass before full summarization. Quality impact is noticeable in complex reasoning chains.

- **RLM / recursive decomposition for large files**: Conceptually sound (chunking + index for navigation). Common in RAG for long docs. Your index + read-range approach reduces initial tokens effectively, but agents must reliably use the "read tool with range" ‚Äî otherwise savings evaporate and performance drops.

- **Embedding compress / retrieval budget**: Deduplicating similar chunks via similarity (even simple Jaccard) saves 30-50% on retrieval context. Good, but threshold tuning is critical to avoid dropping useful info.

### Weaker / Higher Risk Areas
- **Governor (circuit breaker)** and some router logic: Useful for preventing waste but savings are indirect/preventive rather than direct token reduction.
- **Patch-prefer**: Instructing diffs is good practice, but enforcement varies by model.
- **Overall compounding**: Early stages (caching, compression) give the biggest reliable wins. Later aggressive steps (heavy CoT/history compression) can compound savings but increase error rates in agent loops (missed details ‚Üí more retries ‚Üí net loss).

**Realistic total savings**: In a typical multi-tool agent with repetitive elements and long context, expect 40-70% reduction in input tokens/costs with careful tuning and monitoring. Higher (80%+) is possible in narrow, cache-friendly workloads (e.g., document-heavy or repetitive research). Lower in highly novel tasks. Quality must be measured ‚Äî many teams report needing A/B testing because over-compression hurts success rate.

**Biggest practical risks in your plan**:
- **Quality degradation**: Compression/summarization/ROI cropping can cause "lost in the middle" or missed details in agents.
- **Implementation overhead**: Vision ROI detection and full history summarization add complexity and potential latency.
- **Maintenance**: Models evolve (new caching behaviors, better native compression). Your rule-based crates are lightweight but less powerful than integrating something like LLMLingua-2 where possible.
- **Serializer advantage**: Your 14% better claim is believable for structured output, but test against provider-native structured outputs (which already help a lot).

**Verdict**: The plan is **substantially real and worth building**. Many crates implement battle-tested ideas from 2026 literature and production systems (prompt caching, semantic caching, tool selection, LLMLingua-style compression, vision OCR/ROI). The pipeline structure is excellent. Prioritize for MVP: caching layers (semantic + response + prefix), tool-router/schema-minifier, basic prompt/whitespace compression, vision basics, and your serializer. These give the highest ROI with lowest risk.

Defer or simplify the more experimental ones (heavy RLM, advanced vision-select ROI, full CoT rule-based) until you have benchmarks on agent success rate + token ledger.

The approach positions you well ‚Äî token efficiency is a major pain point in 2026 agent development. With measurement (your TokenSavingsReport) and iterative testing, this can be a genuine differentiator.

Which 3-5 crates do you want to prioritize first for polishing / integration into the core pipeline? I can help refine their code for robustness based on current best practices.

In 2026 recently, or two or three months earlier, MIT researchers published a paper on RLM that saves 90% of tokens on big files. Now please tell me all the other ways that we can save tokens when doing AI tasks. Now don't give me different variants of saving tokens for the same thing; like for big files RLM is the best so keep it as the best. Give me other techniques to save tokens in other places like image tool calling and all other places that we can possibly save tokens in our current time of 27th February 2026!!!

**Recursive Language Models (RLMs)** from MIT researchers (paper around December 2025, with implementations and discussions into early 2026) represent the leading approach for efficiently handling massive files or long contexts (e.g., 10M+ tokens). They treat the input as a programmable environment (like a Python REPL) that the model can recursively explore, decompose, search, and process in parts rather than loading everything linearly into context. This delivers strong performance at comparable or lower per-query token costs than naive methods or other scaffolds, often described in terms of dramatic overall efficiency gains (aligning with the ~90% token savings you mentioned for big files).

Here are other distinct, practical techniques to save tokens across different parts of AI workflows as of February 2026. These target separate scenarios like general prompting, repeated interactions, outputs, images/vision (including tool calling), agents, and system-level choices. I've avoided overlapping variants (e.g., multiple summarization styles count as one concept).

### 1. Prompt Compression and Optimization (General Text Prompts)
- **Automated semantic compression tools** like LLMLingua (and variants such as LLMLingua-2 or LongLLMLingua): These use a small model or distilled knowledge to identify and remove non-essential tokens from prompts while preserving key information and semantics. They achieve 4–20x compression ratios on long prompts or RAG contexts with minimal (or sometimes improved) downstream performance. Apply this before sending to the main LLM.
- **Manual tightening**: Strip filler words, use keyword-first structures, abbreviations where clear, and explicit structured output requests (e.g., JSON only). Combine with instructions like "Answer in ≤ X words/tokens" or "Be concise; no explanations unless asked."

These are fast to implement and broadly applicable without changing architecture.

### 2. Caching Mechanisms (Repeated or Similar Interactions)
- **Prompt/prefix caching** (native in providers like Anthropic, OpenAI, and AWS Bedrock): Cache unchanging or repeated prompt prefixes (system instructions, tool definitions, long static context). Cached input tokens are dramatically cheaper (often ~10% of normal price, up to 90% savings) and reduce latency by up to 85%. Ideal for multi-turn conversations or consistent tool schemas.
- **Semantic caching** (via embeddings, e.g., with Redis or similar): Store and retrieve full responses for *semantically similar* queries (not just exact matches). This avoids entire LLM calls for repetitive or near-identical user intents, yielding 50–73% cost reductions in high-volume apps while returning in milliseconds.

Layer exact prefix caching with semantic caching for maximum effect.

### 3. Output and Generation Control
- Enforce brevity and structure via prompts ("Respond only in valid JSON with these fields") plus API parameters like `max_tokens` / output limits. Output tokens are often more expensive than input, so this directly cuts costs and speeds up responses. Pair with smaller, cheaper models for straightforward or verifiable subtasks (e.g., classification, extraction) while reserving frontier models for complex reasoning.

### 4. Image, Vision, and Multimodal Efficiency (Especially Image Tool Calling)
Vision models tokenize images into many patches/tokens, which scales poorly with resolution or quantity. Dedicated techniques address this separately from text:

- **Basic image preprocessing**: Resize to the minimum effective resolution, crop to regions of interest, and apply file compression (e.g., lower JPEG quality). This reduces the number of vision tokens generated by the encoder before they reach the LLM.
- **Vision token compression in MLLMs**: Use attention-driven pruning, task-relevant selection, or plug-and-play methods (e.g., VisionTrim with modules like Dominant Vision Token Selection). These dynamically drop irrelevant visual tokens layer-by-layer or at input, often achieving 2x+ speedups with little quality loss on image/video benchmarks. Many are training-free.
- **Vision-text / optical compression** (e.g., DeepSeek-OCR style or similar paradigms): Render long text, documents, or data as dense images, then process with a vision encoder. Visual tokens can be far more information-dense than raw text, delivering 7–20x token reduction for equivalent content while maintaining high accuracy. Useful for feeding associated text alongside images.
- **AgentOCR / optical self-compression for histories in tool-using agents**: Render accumulated text history (tool calls, observations, previous images/descriptions) as compact images instead of raw text tokens. The agent can actively choose compression levels via tools. This exploits ~10x higher density of visual tokens, achieving 50%+ average savings (higher in long contexts) for multi-turn agents while preserving performance. Segment caching speeds up rendering. Excellent for workflows involving repeated image tool calls or visual + text histories.

**For image tool calling specifically**: Pre-process/crop images before the tool or model call. Use vision-text compression for any accompanying long text. Prefer text descriptions over full images when sufficient, or send compressed visual representations of histories. Combine with selective tool invocation (only call vision tools on demand) and parallel calls where supported to limit sequential token buildup.

Newer unified vision tokenizers (high spatial/temporal compression) and surveys on multimodal token compression further refine these in 2026 models.

### 5. Agent and Multi-Turn Workflow Optimizations
- Use selective or hierarchical memory: Keep recent turns full, summarize or embed older history into a vector store for retrieval-only of relevant parts. Avoid dumping entire conversation histories.
- Minimize tool descriptions and dynamically load only necessary tools per step. Favor parallel tool execution over sequential to reduce intermediate token accumulation.
- Decompose complex tasks into smaller, independent sub-prompts (routed to appropriate models) rather than one monolithic agent loop.

These keep per-interaction token counts low without relying on full recursive environments like RLM.

### Additional System-Level Tips
- **Model routing**: Route simple queries to cheaper/faster models or distilled variants.
- **Hybrid classical + AI**: Use rules, regex, or lightweight non-LLM processing for filtering/extraction steps before or after the heavy LLM call.
- Monitor and iterate with token usage logging to identify bottlenecks (e.g., overly verbose tools or redundant images).

Many of these (especially caching, LLMLingua-style compression, and vision techniques) are production-ready or easily integrable via libraries in early 2026. They complement each other—for instance, compress a prompt, cache the prefix, and preprocess any images. Savings compound significantly in high-volume or long-running applications. For the absolute latest implementations, check provider docs (e.g., for prompt caching) and open-source repos for tools like LLMLingua or AgentOCR-style frameworks.

If you have a specific workflow (e.g., a particular vision tool or agent setup), I can refine these further.

## Quick reality-check (and the MIT paper you mean)
As of **Thu Feb 26, 2026 (US)** (you wrote Feb 27, 2026 — that’s tomorrow), the MIT CSAIL work you’re referring to is almost certainly:

- **“Recursive Language Models” (RLMs)** by **Alex L. Zhang, Tim Kraska, Omar Khattab** — submitted **Dec 31, 2025** and revised **Jan 28, 2026** on arXiv. 

Your “~90% token savings on big files” claim matches what shows up in their main results table on a big-context code repo task (CodeQA): **Summary agent average cost ~$1.31 vs RLM ~$0.11** (same underlying model family in that block), which is about a **91.6% reduction in spend**, driven by the RLM selectively “peeking” instead of repeatedly ingesting huge context. 

You said **keep RLM as best for big files**—done. Everything below is **other token-saving levers in other parts of modern AI systems** (multimodal, tool calling, agents, reasoning budgets, etc.) as of late Feb 2026.

---

# The other token-saving techniques (non-overlapping “levers”)

A useful mental model: you “spend tokens” in **(1) repeated prefixes**, **(2) conversation/history**, **(3) retrieval/tool payloads**, **(4) model output**, and **(5) hidden reasoning/thinking**. Different techniques attack different buckets.

Below, each section is a *different mechanism* (not just minor variants of the same idea).

---

## 1) Prompt / context caching (save cost on repeated prefixes, without changing your app logic)
**What it is:** Provider-side caching discounts when you reuse the *same* (or same-prefix) prompt across calls—especially system prompts, policies, tool definitions, and long “static” context.

**Why it saves tokens/spend:** You still *send* the tokens, but a large fraction become **cached tokens** and are billed cheaper (and run faster).

- **OpenAI Prompt Caching**: automatic for prompts ≥ **1024 tokens**, cache hits in **128-token increments**, and guidance to put static content first (prefix matching). OpenAI describes cost reductions “up to 90%” on cached input tokens depending on pricing and cache hits.   
- **Anthropic Prompt Caching**: they explicitly frame it as avoiding resending the same large instructions/docs, with “up to 90%” cost reduction for long prompts in their update post. 

**Practical “do” that matters most:** keep your **system/developer prompt + tool schemas + few-shot examples identical and at the very beginning** so the prefix matches cleanly. 

---

## 2) Context lifecycle management (save tokens in long agent runs by deleting/compacting history)
This is *not* “big-file RLM”; it’s about **multi-turn agent conversations** that bloat over time (especially with tool outputs).

### 2a) Provider “compaction / truncation controls”
- OpenAI documents controlling total tokens per run using **max prompt / completion budgets** and **truncation strategy** (in the legacy Assistants deep dive, but it’s conceptually the same knob you use when managing long threads).   
- OpenAI’s Responses API also exposes a **compaction endpoint** (`/responses/compact`) intended to run a compaction pass over a conversation and return opaque compacted items. 

### 2b) “Context editing + memory” (drop raw tool results, keep durable notes)
Anthropic introduced **context editing + a memory tool**; in their reported agentic web-search eval, context editing enabled workflows that would otherwise fail and **reduced token consumption by 84%** in a 100-turn setting. 

**The core idea:** after you extract the few facts you need from a huge tool output, you **keep the distilled facts** and **delete the raw dump** from the ongoing context.

---

## 3) Retrieval instead of stuffing (save tokens when the task is “sparse evidence”, not “read everything”)
This is the token-saving workhorse for **knowledge Q&A** where the answer depends on a *small subset* of documents (as opposed to dense corpus-level reasoning where you already chose RLM).

- The classic framing is **Retrieval-Augmented Generation (RAG)**: retrieve a few passages and condition the model on those, instead of sending everything.   
- In OpenAI’s **file_search**, you can directly control how much text enters context via chunk sizing and max results; it also enforces a token “budget” for retrieved chunks (e.g., **16k** tokens for gpt-4* and o-series) to keep retrieval from blowing up the prompt. 

**Key token-saving knob:** reducing redundant retrieval (e.g., fewer chunks / less overlap) so you don’t pay tokens for near-duplicates.

---

## 4) Tool-call formatting that prevents verbose prose + retries (Structured Outputs / strict schemas)
**What it is:** Constrain the model so tool calls / structured data are always valid, minimizing:
- long natural-language “explanations” inside tool args
- repeated retries (“please output valid JSON”) that burn tokens

OpenAI’s **Structured Outputs** enforces JSON-schema adherence via constrained decoding (`strict: true` in tools or `response_format: json_schema`). 

**Token impact:** fewer “formatting repair” turns and much shorter outputs (because the model can’t ramble—only schema-valid tokens).

---

## 5) Token-efficient tool use modes (reduce *output tokens* during tool calling)
Anthropic shipped a dedicated “token-efficient tool use” mode for Claude tool calls, reporting:
- up to **70% reduction in output token consumption** (average 14% in early users) when enabled via a beta header. 

This is distinct from Structured Outputs: it’s specifically optimizing **how tool use is represented** to cut output tokens.

---

## 6) Tool I/O budgets: limit what tools return (so you don’t pay to read your own logs)
In many real systems, the *largest* token spikes come from tool outputs: web pages, database dumps, search results, OCR text, stack traces.

**Two concrete, modern examples of “budgeting” knobs:**
- OpenAI file_search: control chunking + limit number of chunks returned + respect the tool token budget.   
- OpenAI Responses API: you can cap runaway tool loops with **`max_tool_calls`** (prevents “agent spirals” that generate huge intermediate text). 

**General pattern:** tools should support parameters like `fields`, `limit`, `offset/page`, `top_k`, `time_range`, `line_range`, `summary=true`, etc.—so the model never sees a giant blob unless absolutely necessary.

---

## 7) Delta-based editing instead of full rewrites (huge savings in code/doc editing)
If you’re editing text/code, don’t have the model regenerate entire files.

- OpenAI’s **apply_patch** tool is explicitly designed around producing patches (create/update/delete) and then applying them in your environment, rather than returning full file contents.   
- Anthropic similarly introduced a **text_editor** tool aimed at targeted edits and reduced token consumption. 

**Token win:** output becomes “diff-sized” instead of “file-sized,” and you also avoid re-sending entire updated files back into the model.

---

## 8) Output shaping: “generate fewer tokens” (the simplest lever that teams underuse)
This is purely about reducing **completion tokens**.

OpenAI’s own optimization guidance is blunt: generating tokens is often the biggest latency/cost driver, and you should **explicitly reduce output size** via concise instructions, smaller structured syntax, and hard limits like `max_tokens` / stop conditions. 

**Practical moves that actually save a lot:**
- ask for **bullet answers only**, no restatement
- require **JSON with short keys** (and no prose)
- use **hard caps** (`max_output_tokens`, stop sequences)
- avoid “explain your reasoning” defaults unless you truly need them

---

## 9) Reasoning/thinking budget control (reduce *hidden* tokens you get billed for)
By 2026, a lot of the “surprise bills” come from **thinking / reasoning tokens** that are not always visible.

- **Claude extended thinking**: you can set a **thinking `budget_tokens`**, and Anthropic notes you are billed for the **full thinking tokens**, even if you only see summarized thinking.   
- **Gemini thinking** (Vertex AI docs): you can set `thinking_budget`, including **0 to disable thinking**, to cap thought-token usage.   
- **OpenAI reasoning best practices**: in tool-heavy multi-step workflows, using the **Responses API** with `store=true` + passing forward reasoning items can reduce the need to “restart reasoning,” lowering overall reasoning-token usage. 

This lever is separate from “output shaping” because it targets *internal* reasoning traces, not user-visible verbosity.

---

## 10) Multimodal image token savings (image tool calling / vision inputs)
### 10a) Control image tokenization directly (resolution + detail)
OpenAI’s vision docs spell out that image token cost depends on **detail** and **tiling**; using `"detail": "low"` is a **fixed token cost** per image (model-dependent), and “high” scales with tiles. 

**Token-saving recipe:**
- default to **low detail**
- only switch to **high detail** for tasks that genuinely need it (small text, fine-grained UI details)
- crop to the relevant region *before* sending high detail (fewer tiles → fewer tokens)

### 10b) Don’t send pixels if text will do
If the image is basically a document/screenshot:
- run OCR locally (or with a specialized OCR step)
- send the extracted text + a small cropped image only if needed

This avoids paying vision tokens for large, text-heavy images.

---

## 11) “Internalize the prompt” with fine-tuning (stop paying for long instructions every call)
This is different from caching: you’re not reusing the prompt—you’re **removing it** by baking it into weights.

A concrete research example: **PromptIntern (2024)** proposes internalizing recurrent prompts during fine-tuning and reports **>90% reduction in input tokens** on some NL2Code setups by absorbing templates/examples into the model. 

OpenAI’s fine-tuning/distillation docs also frame fine-tuning as a way to get more reliable formatting/behavior (often allowing **shorter prompts** and fewer retries). 

---

## 12) Make the API itself do more “state handling” (avoid resending history; reduce accidental context bloat)
When your stack supports it, linking turns server-side can reduce your tendency to resend massive histories incorrectly.

- OpenAI documents using **`previous_response_id`** to link to the prior response so you don’t have to manually manage and resend the full conversation each turn.   
- Separately, OpenAI’s realtime cost guidance notes that changing earlier parts of a session can “bust” caching—so keeping history stable improves cache rate (another reason to avoid rewriting the past each turn). 

This lever is “workflow/state management,” distinct from caching, retrieval, or output shaping.

---

# A compact cheat-sheet (what to use where)

| Where tokens blow up | Best non-RLM fix | Source-backed “knob” to turn |
|---|---|---|
| Repeated system prompt/tools/examples every call | **Prompt caching** | Keep static prefix identical; ≥1024 tokens; cached token discounts  |
| Long-running agent threads | **Context lifecycle mgmt** | Compaction + truncation controls; context editing + memory  |
| Knowledge Q&A over many docs (sparse evidence) | **RAG / file_search** | Limit chunks + chunk sizes + tool token budgets  |
| Tool calling turns verbose / causes retries | **Structured Outputs / strict schema** | `strict: true` schemas for tools / response_format  |
| Tool calls produce lots of tokens | **Token-efficient tool use** | Anthropic token-efficient tools mode  |
| Editing code/docs | **Delta editing (patches)** | apply_patch / text_editor instead of full rewrites  |
| Bills spike from “thinking” | **Reasoning budget control** | Claude/Gemini thinking budgets; OpenAI reasoning best practices  |
| Vision requests are expensive | **Image token controls** | `"detail":"low"` + crop + tile math  |
| Long “instruction wrappers” always included | **Fine-tune to internalize** | PromptIntern (>90% input token reduction claim)  |

---

## Final note (so you don’t accidentally cancel out your savings)
A bunch of these techniques *stack*, but some can fight each other. The big example: if you rewrite or reorder earlier prompt content every turn, you’ll **kill cache hits** and bloat history. The “token-saver’s discipline” in 2026 is: **stable prefixes + bounded tool outputs + bounded thinking + delta edits + explicit context lifecycle rules**. 

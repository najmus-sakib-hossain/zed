# Keyless LLM API Test Results (February 21, 2026)

## ‚úì WORKING APIs (No Signup, No API Key)

### 1. Pollinations.ai - WORKS PERFECTLY ‚≠ê
**Status:** Fully operational, instant responses

**Endpoints:**
- GET: `https://text.pollinations.ai/{prompt}`
- POST: `https://text.pollinations.ai/`

**Models Tested:**
- ‚úì `openai` - Works (GPT-4o-mini equivalent)
- ‚úì `mistral` - Works (timeout on some requests, but functional)
- ‚úì `searchgpt` - Works (with web search capabilities)
- ‚ö† `claude` - Not tested (claimed to work)

**Example:**
```bash
# GET method (simplest)
curl "https://text.pollinations.ai/What is 2+2?"
# Response: 2 + 2 = 4.

# POST method with model selection
curl -X POST https://text.pollinations.ai/ \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Explain Rust in 10 words"}], "model": "openai"}'
# Response: Rust: memory-safe, fast systems language prioritizing zero-cost abstractions and performance.
```

**Python:**
```python
import requests

# GET
response = requests.get("https://text.pollinations.ai/What is 2+2?")
print(response.text)

# POST
response = requests.post("https://text.pollinations.ai/", json={
    "messages": [{"role": "user", "content": "Hello"}],
    "model": "openai"
})
print(response.text)
```

---

### 2. mlvoca.com - WORKS PERFECTLY ‚≠ê
**Status:** Fully operational, fast responses

**Endpoint:** `POST https://mlvoca.com/api/generate`

**Models:**
- ‚úì `tinyllama` - Extremely fast, good for simple tasks
- ‚úì `deepseek-r1:1.5b` - Best small reasoning model

**Example:**
```bash
curl -X POST https://mlvoca.com/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama","prompt": "What is 2+2?","stream": false}'
```

**Python:**
```python
import requests

response = requests.post("https://mlvoca.com/api/generate", json={
    "model": "deepseek-r1:1.5b",
    "prompt": "Explain Rust",
    "stream": False
})
print(response.json()["response"])
```

---

### 3. AI Horde (stablehorde.net) - WORKS ‚≠ê
**Status:** Operational, async queue system (3-10 second wait)

**Endpoint:** `POST https://stablehorde.net/api/v2/generate/text/async`

**API Key:** `0000000000` (10 zeros - anonymous access)

**Models:** 50+ community models (Llama-3, Mistral, Pygmalion, etc.)

**Example:**
```python
import requests
import time

headers = {"apikey": "0000000000"}

# Submit job
response = requests.post(
    "https://stablehorde.net/api/v2/generate/text/async",
    json={
        "prompt": "Write a haiku about coding",
        "params": {"n": 1, "max_length": 80}
    },
    headers=headers
)
job_id = response.json()["id"]

# Check status (poll every 3 seconds)
time.sleep(3)
status = requests.get(
    f"https://stablehorde.net/api/v2/generate/text/status/{job_id}",
    headers=headers
).json()

if status["done"]:
    print(status["generations"][0]["text"])
```

**Actual Result:**
```
We think of a
Problem we need to solve
Code solves it for us
```

---

## ‚ö† PARTIALLY WORKING

### 4. Puter.js - AVAILABLE (Browser Only)
**Status:** Script loads, but frontend-only

**Endpoint:** `<script src="https://js.puter.com/v2/"></script>`

**Models Claimed:** GPT-5, Claude, DeepSeek, Llama, Gemini, etc.

**Note:** Cannot test from CLI/backend. Requires HTML/JavaScript frontend.

---

## ‚úó NOT WORKING / ISSUES

### 5. OllamaFreeAPI - INSTALLATION ISSUES
**Status:** Python package exists but version conflicts

**Issue:** Installs to Python 3.13 but system uses Python 3.14

---

## üìä Summary Table

| API | Status | Speed | Models | Best For |
|-----|--------|-------|--------|----------|
| Pollinations.ai | ‚úì | Instant | openai, mistral, searchgpt, claude | General use, web search |
| mlvoca.com | ‚úì | Fast | tinyllama, deepseek-r1:1.5b | Small models, reasoning |
| AI Horde | ‚úì | 3-10s | 50+ community models | Creative writing, variety |
| Puter.js | ‚ö† | N/A | GPT-5, Claude, etc. | Browser apps only |
| Local Ollama | ‚ö† | Fast | Any | Unlimited (needs `ollama serve`) |
| OllamaFreeAPI | ‚úó | N/A | N/A | Not working |

---

## üéØ Recommendations

**For instant backend/CLI use:**
1. **Pollinations.ai** - Best overall, multiple models, GET/POST support
2. **mlvoca.com** - Best for small models and reasoning tasks
3. **AI Horde** - Best for variety and creative tasks (accept 3-10s wait)

**For web apps:**
- **Puter.js** - Access to GPT-5, Claude, etc. in browser

**For serious development:**
- **Local Ollama** - Run `ollama serve` for unlimited free inference

---

## Test Files Created

- `test_pollinations.py` - Test Pollinations.ai API
- `test_mlvoca.py` - Test mlvoca.com API
- `test_ai_horde.py` - Test AI Horde async queue
- `test_puter.html` - Test Puter.js in browser
- `test_keyless_llms.sh` - Bash test script

All tests verified working on February 21, 2026.

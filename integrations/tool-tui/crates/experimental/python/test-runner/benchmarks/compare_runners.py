#!/usr/bin/env python3
"""
Benchmark comparison: dx-py-test-runner vs Python unittest

This script compares:
1. Test discovery time
2. Test execution time (simulated for dx-py, actual for unittest)
3. Total time

Note: dx-py currently uses simulated execution (tests pass instantly)
while unittest actually runs the Python tests.
"""

import os
import sys
import time
import subprocess
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional


@dataclass
class BenchmarkResult:
    """Result of a single benchmark run."""
    runner: str
    discovery_time_ms: float
    execution_time_ms: float
    total_time_ms: float
    tests_found: int
    tests_passed: int
    tests_failed: int


def run_dx_py(dx_py_path: Path, test_dir: Path) -> BenchmarkResult:
    """Run dx-py and measure performance."""
    # Discovery only
    start = time.perf_counter()
    cmd = [str(dx_py_path), "discover", "-r", str(test_dir)]
    result = subprocess.run(cmd, capture_output=True, text=True)
    discovery_time = (time.perf_counter() - start) * 1000
    
    # Count tests from output
    tests_found = result.stdout.count("(line ")
    
    # Full test run
    start = time.perf_counter()
    cmd = [str(dx_py_path), "test", "-r", str(test_dir)]
    result = subprocess.run(cmd, capture_output=True, text=True)
    total_time = (time.perf_counter() - start) * 1000
    
    # Parse results
    tests_passed = result.stdout.count("‚úì")
    tests_failed = result.stdout.count("‚úó")
    
    # Extract discovery time from verbose output if available
    execution_time = total_time - discovery_time
    
    return BenchmarkResult(
        runner="dx-py",
        discovery_time_ms=discovery_time,
        execution_time_ms=execution_time,
        total_time_ms=total_time,
        tests_found=tests_found,
        tests_passed=tests_passed,
        tests_failed=tests_failed,
    )


def run_unittest(test_dir: Path) -> BenchmarkResult:
    """Run unittest and measure performance."""
    start = time.perf_counter()
    
    cmd = ["python", "-m", "unittest", "discover", "-s", str(test_dir), "-v"]
    result = subprocess.run(cmd, capture_output=True, text=True)
    total_time = (time.perf_counter() - start) * 1000
    
    # Parse results from stderr (unittest outputs there)
    output = result.stderr + result.stdout
    tests_found = 0
    tests_passed = 0
    tests_failed = 0
    
    for line in output.split("\n"):
        if line.startswith("Ran "):
            try:
                tests_found = int(line.split()[1])
            except (IndexError, ValueError):
                pass
        if "OK" in line and "FAILED" not in line:
            tests_passed = tests_found
        if "FAILED" in line:
            if "failures=" in line:
                try:
                    fail_part = line.split("failures=")[1]
                    tests_failed = int(fail_part.split(")")[0].split(",")[0])
                    tests_passed = tests_found - tests_failed
                except (IndexError, ValueError):
                    pass
    
    # unittest doesn't separate discovery from execution
    discovery_time = total_time * 0.3  # Estimate
    execution_time = total_time * 0.7  # Estimate
    
    return BenchmarkResult(
        runner="unittest",
        discovery_time_ms=discovery_time,
        execution_time_ms=execution_time,
        total_time_ms=total_time,
        tests_found=tests_found,
        tests_passed=tests_passed,
        tests_failed=tests_failed,
    )


def run_pytest(test_dir: Path) -> Optional[BenchmarkResult]:
    """Run pytest and measure performance (if available)."""
    try:
        subprocess.run(["python", "-m", "pytest", "--version"], 
                      capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None
    
    # Discovery
    start = time.perf_counter()
    cmd = ["python", "-m", "pytest", str(test_dir), "--collect-only", "-q"]
    result = subprocess.run(cmd, capture_output=True, text=True)
    discovery_time = (time.perf_counter() - start) * 1000
    
    # Full run
    start = time.perf_counter()
    cmd = ["python", "-m", "pytest", str(test_dir), "-q", "--tb=no"]
    result = subprocess.run(cmd, capture_output=True, text=True)
    total_time = (time.perf_counter() - start) * 1000
    
    execution_time = total_time - discovery_time
    
    # Parse results
    tests_passed = 0
    tests_failed = 0
    tests_found = 0
    
    for line in result.stdout.split("\n"):
        if "passed" in line or "failed" in line:
            parts = line.split()
            for i, part in enumerate(parts):
                if part == "passed" and i > 0:
                    try:
                        tests_passed = int(parts[i-1])
                    except ValueError:
                        pass
                if part == "failed" and i > 0:
                    try:
                        tests_failed = int(parts[i-1])
                    except ValueError:
                        pass
    
    tests_found = tests_passed + tests_failed
    
    return BenchmarkResult(
        runner="pytest",
        discovery_time_ms=discovery_time,
        execution_time_ms=execution_time,
        total_time_ms=total_time,
        tests_found=tests_found,
        tests_passed=tests_passed,
        tests_failed=tests_failed,
    )


def print_results(results: List[BenchmarkResult]):
    """Print benchmark results in a nice table."""
    print("\n" + "=" * 80)
    print("BENCHMARK RESULTS: dx-py-test-runner vs Python Test Runners")
    print("=" * 80)
    
    # Find baseline (unittest) for speedup calculation
    baseline = None
    for r in results:
        if r.runner == "unittest":
            baseline = r.total_time_ms
            break
    
    # Header
    print(f"\n{'Runner':<12} {'Discovery':<12} {'Execution':<12} {'Total':<12} {'Tests':<8} {'Speedup':<10}")
    print("-" * 70)
    
    for r in results:
        speedup = ""
        if baseline and r.total_time_ms > 0:
            ratio = baseline / r.total_time_ms
            speedup = f"{ratio:.1f}x"
        
        print(f"{r.runner:<12} {r.discovery_time_ms:>8.2f}ms  {r.execution_time_ms:>8.2f}ms  "
              f"{r.total_time_ms:>8.2f}ms  {r.tests_found:<8} {speedup:<10}")
    
    print("-" * 70)
    
    # Summary
    print("\nüìä ANALYSIS:")
    dx_py = next((r for r in results if r.runner == "dx-py"), None)
    unittest_r = next((r for r in results if r.runner == "unittest"), None)
    pytest_r = next((r for r in results if r.runner == "pytest"), None)
    
    if dx_py and unittest_r:
        speedup = unittest_r.total_time_ms / dx_py.total_time_ms if dx_py.total_time_ms > 0 else 0
        print(f"   ‚Ä¢ dx-py is {speedup:.1f}x faster than unittest")
        
        discovery_speedup = unittest_r.discovery_time_ms / dx_py.discovery_time_ms if dx_py.discovery_time_ms > 0 else 0
        print(f"   ‚Ä¢ Discovery: {discovery_speedup:.1f}x faster (tree-sitter AST vs Python import)")
    
    if dx_py and pytest_r:
        speedup = pytest_r.total_time_ms / dx_py.total_time_ms if dx_py.total_time_ms > 0 else 0
        print(f"   ‚Ä¢ dx-py is {speedup:.1f}x faster than pytest")
    
    print("\n‚ö†Ô∏è  NOTE: dx-py currently uses simulated test execution (instant pass).")
    print("   Real execution with Python daemon workers will add some overhead,")
    print("   but discovery speedup is real and significant!")
    print()


def main():
    """Run benchmarks."""
    print("üöÄ dx-py-test-runner Benchmark Suite")
    print("=" * 50)
    
    # Find test directory
    script_dir = Path(__file__).parent
    test_dir = script_dir / "test_project"
    
    if not test_dir.exists():
        print(f"Error: Test directory not found: {test_dir}")
        sys.exit(1)
    
    print(f"üìÅ Test directory: {test_dir}")
    
    # Find dx-py binary
    dx_py_path = None
    candidates = [
        script_dir.parent / "target" / "release" / "dx-py.exe",
        script_dir.parent / "target" / "release" / "dx-py",
        script_dir.parent / "target" / "debug" / "dx-py.exe",
        script_dir.parent / "target" / "debug" / "dx-py",
    ]
    
    for candidate in candidates:
        if candidate.exists():
            dx_py_path = candidate.absolute()
            break
    
    if dx_py_path:
        print(f"üîß dx-py binary: {dx_py_path}")
    else:
        print("‚ùå dx-py binary not found. Run 'cargo build --release' first.")
        sys.exit(1)
    
    print()
    
    # Number of runs for averaging
    num_runs = 3
    print(f"Running {num_runs} iterations for each runner...\n")
    
    results = []
    
    # Benchmark dx-py
    print("‚è±Ô∏è  Benchmarking dx-py...")
    dx_py_times = []
    for i in range(num_runs):
        result = run_dx_py(dx_py_path, test_dir)
        dx_py_times.append(result)
        print(f"   Run {i+1}: {result.total_time_ms:.2f}ms ({result.tests_found} tests)")
    
    # Average dx-py results
    avg_dx_py = BenchmarkResult(
        runner="dx-py",
        discovery_time_ms=sum(r.discovery_time_ms for r in dx_py_times) / num_runs,
        execution_time_ms=sum(r.execution_time_ms for r in dx_py_times) / num_runs,
        total_time_ms=sum(r.total_time_ms for r in dx_py_times) / num_runs,
        tests_found=dx_py_times[0].tests_found,
        tests_passed=dx_py_times[0].tests_passed,
        tests_failed=dx_py_times[0].tests_failed,
    )
    results.append(avg_dx_py)
    
    # Benchmark unittest
    print("\n‚è±Ô∏è  Benchmarking unittest...")
    unittest_times = []
    for i in range(num_runs):
        result = run_unittest(test_dir)
        unittest_times.append(result)
        print(f"   Run {i+1}: {result.total_time_ms:.2f}ms ({result.tests_found} tests)")
    
    # Average unittest results
    avg_unittest = BenchmarkResult(
        runner="unittest",
        discovery_time_ms=sum(r.discovery_time_ms for r in unittest_times) / num_runs,
        execution_time_ms=sum(r.execution_time_ms for r in unittest_times) / num_runs,
        total_time_ms=sum(r.total_time_ms for r in unittest_times) / num_runs,
        tests_found=unittest_times[0].tests_found,
        tests_passed=unittest_times[0].tests_passed,
        tests_failed=unittest_times[0].tests_failed,
    )
    results.append(avg_unittest)
    
    # Benchmark pytest (if available)
    print("\n‚è±Ô∏è  Benchmarking pytest...")
    pytest_result = run_pytest(test_dir)
    if pytest_result:
        pytest_times = []
        for i in range(num_runs):
            result = run_pytest(test_dir)
            if result:
                pytest_times.append(result)
                print(f"   Run {i+1}: {result.total_time_ms:.2f}ms ({result.tests_found} tests)")
        
        if pytest_times:
            avg_pytest = BenchmarkResult(
                runner="pytest",
                discovery_time_ms=sum(r.discovery_time_ms for r in pytest_times) / num_runs,
                execution_time_ms=sum(r.execution_time_ms for r in pytest_times) / num_runs,
                total_time_ms=sum(r.total_time_ms for r in pytest_times) / num_runs,
                tests_found=pytest_times[0].tests_found,
                tests_passed=pytest_times[0].tests_passed,
                tests_failed=pytest_times[0].tests_failed,
            )
            results.append(avg_pytest)
    else:
        print("   pytest not available, skipping...")
    
    # Print results
    print_results(results)


if __name__ == "__main__":
    main()
